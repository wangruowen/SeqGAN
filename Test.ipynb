{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import linecache\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "\n",
    "from keras.utils import Sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SeqGAN.train import *\n",
    "from SeqGAN.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config ={\n",
    "    'batch_size': 128,\n",
    "    'max_length': 15,\n",
    "    'max_words': 10000,\n",
    "    'gen_embed': 100,\n",
    "    'dis_embed': 100,\n",
    "    'gen_hidden': 128,\n",
    "    'dis_hidden': 128,\n",
    "    'dis_dropout': 0.2,\n",
    "    'gen_lr': 1e-3,\n",
    "    'dis_lr': 1e-3,\n",
    "    'mcts_sample': 10,\n",
    "    'gen_samples': 5000,\n",
    "    'gen_sample_length': 50,  # Generated sample can be longer than max_length\n",
    "\n",
    "    'rnn_layers': 3,\n",
    "    'rnn_bidirectional': False,\n",
    "\n",
    "    'gen_pretrain_epoch': 10,\n",
    "    'dis_pretrain_epoch': 1,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word2id:  10004\n",
      "Total id2word:  10004\n",
      "Num of positive training:  398450\n",
      "Num of positive validation:  0\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(config, \"./data/just_tweet_without_I_will_be_interviewed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator pre-training\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "Embedding (Embedding)        (None, 15, 100)           1000400   \n",
      "_________________________________________________________________\n",
      "rnn_1 (CuDNNLSTM)            (None, 15, 128)           117760    \n",
      "_________________________________________________________________\n",
      "rnn_2 (CuDNNLSTM)            (None, 15, 128)           132096    \n",
      "_________________________________________________________________\n",
      "rnn_3 (CuDNNLSTM)            (None, 128)               132096    \n",
      "_________________________________________________________________\n",
      "DenseSoftmax (Dense)         (None, 10004)             1290516   \n",
      "=================================================================\n",
      "Total params: 2,672,868\n",
      "Trainable params: 2,672,868\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "3112/3112 [==============================] - 29s 9ms/step - loss: 6.2020\n",
      "Epoch 2/20\n",
      "3112/3112 [==============================] - 28s 9ms/step - loss: 5.4769\n",
      "Epoch 3/20\n",
      "3112/3112 [==============================] - 28s 9ms/step - loss: 5.0925\n",
      "Epoch 4/20\n",
      "3112/3112 [==============================] - 28s 9ms/step - loss: 4.8446\n",
      "Epoch 5/20\n",
      "3112/3112 [==============================] - 28s 9ms/step - loss: 4.6495\n",
      "Epoch 6/20\n",
      "3112/3112 [==============================] - 28s 9ms/step - loss: 4.4899\n",
      "Epoch 7/20\n",
      "3112/3112 [==============================] - 28s 9ms/step - loss: 4.3536\n",
      "Epoch 8/20\n",
      "3112/3112 [==============================] - 28s 9ms/step - loss: 4.2351\n",
      "Epoch 9/20\n",
      "3112/3112 [==============================] - 29s 9ms/step - loss: 4.1268\n",
      "Epoch 10/20\n",
      "3112/3112 [==============================] - 28s 9ms/step - loss: 4.0298\n",
      "Epoch 11/20\n",
      "3112/3112 [==============================] - 28s 9ms/step - loss: 3.9405\n",
      "Epoch 12/20\n",
      "3112/3112 [==============================] - 28s 9ms/step - loss: 3.8565\n",
      "Epoch 13/20\n",
      "3112/3112 [==============================] - 28s 9ms/step - loss: 3.7794\n",
      "Epoch 14/20\n",
      "3112/3112 [==============================] - 28s 9ms/step - loss: 3.7067\n",
      "Epoch 15/20\n",
      "3112/3112 [==============================] - 28s 9ms/step - loss: 3.6356\n",
      "Epoch 16/20\n",
      "3112/3112 [==============================] - 28s 9ms/step - loss: 3.5696\n",
      "Epoch 17/20\n",
      "3112/3112 [==============================] - 28s 9ms/step - loss: 3.5049\n",
      "Epoch 18/20\n",
      "3112/3112 [==============================] - 28s 9ms/step - loss: 3.4452\n",
      "Epoch 19/20\n",
      "3112/3112 [==============================] - 28s 9ms/step - loss: 3.3863\n",
      "Epoch 20/20\n",
      "3112/3112 [==============================] - 28s 9ms/step - loss: 3.3301\n"
     ]
    }
   ],
   "source": [
    "trainer.pre_train_generator(g_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Generating 5000 sentences\n",
      "Num of all training:  505912\n",
      "Num of all validation:  0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "Embedding (Embedding)        (None, 15, 100)           1000400   \n",
      "_________________________________________________________________\n",
      "rnn_1 (CuDNNLSTM)            (None, 15, 128)           117760    \n",
      "_________________________________________________________________\n",
      "rnn_2 (CuDNNLSTM)            (None, 15, 128)           132096    \n",
      "_________________________________________________________________\n",
      "rnn_3 (CuDNNLSTM)            (None, 128)               132096    \n",
      "_________________________________________________________________\n",
      "Dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "FC (Dense)                   (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,382,481\n",
      "Trainable params: 1,382,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Discriminator pre-training\n",
      "Epoch 1/1\n",
      "3952/3952 [==============================] - 23s 6ms/step - loss: 0.2092\n"
     ]
    }
   ],
   "source": [
    "trainer.pre_train_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****GAN step:  0\n",
      "Train Generator at round  0\n",
      "Reward: 0.595, Episode end\n",
      "Loss:  11.029582\n",
      "[1, 1535, 12, 3849, 1819, 8, 7403, 5, 180, 4, 949, 13, 35, 264, 74, 1074]\n",
      "<BOS> forever is approximately guilty and locally the year . couldn ' t believe many evidence\n",
      "[1, 9151, 26, 299, 25, 51, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<BOS> charts it does that ? <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "[1, 5898, 1472, 17, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<BOS> farrell double - <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "--------------------------------------------------------------------------------\n",
      "Train Discriminator at round  0\n",
      "Start Generating 5000 sentences\n",
      "Num of all training:  533158\n",
      "Num of all validation:  0\n",
      "Epoch 1/1\n",
      "4165/4165 [==============================] - 25s 6ms/step - loss: 0.1549\n",
      "****GAN step:  1\n",
      "Train Generator at round  0\n",
      "Reward: 0.300, Episode end\n",
      "Loss:  5.2494445\n",
      "[1, 8304, 9294, 36, 279, 99, 31, 15, 816, 203, 475, 3848, 12, 112, 3495, 11]\n",
      "<BOS> cheater violate by white border & for criminal crime future gains is being renovated in\n",
      "[1, 1649, 4125, 343, 268, 8, 3569, 1251, 58, 6902, 2, 0, 0, 0, 0, 0]\n",
      "<BOS> wallace misrepresents ted cruz and 41 @wsj ) @business <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "[1, 4893, 3935, 56, 7015, 4, 41, 12, 149, 6828, 4891, 4, 400, 46, 731, 38]\n",
      "<BOS> screaming wacko no inspiring . he is $ invaluable contribution . use this dossier they\n",
      "--------------------------------------------------------------------------------\n",
      "Train Discriminator at round  0\n",
      "Start Generating 5000 sentences\n",
      "Num of all training:  561250\n",
      "Num of all validation:  0\n",
      "Epoch 1/1\n",
      "4384/4384 [==============================] - 26s 6ms/step - loss: 0.0919\n",
      "****GAN step:  2\n",
      "Train Generator at round  0\n",
      "Reward: 0.116, Episode end\n",
      "Loss:  2.0592968\n",
      "[1, 3122, 7619, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<BOS> ðŸŽ¥ methods \" <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "[1, 4893, 3935, 102, 29, 385, 3478, 25, 5133, 76, 276, 13, 35, 25, 41, 2250]\n",
      "<BOS> screaming wacko when our vets understands that cowards hillary doesn ' t that he gold\n",
      "[1, 6228, 6767, 224, 17, 2529, 3258, 550, 2424, 446, 57, 114, 753, 149, 1807, 862]\n",
      "<BOS> dynamic remnants before - airport package 13 heritage millions now democrats killed $ hispanic increase\n",
      "--------------------------------------------------------------------------------\n",
      "Train Discriminator at round  0\n",
      "Start Generating 5000 sentences\n",
      "Num of all training:  577267\n",
      "Num of all validation:  0\n",
      "Epoch 1/1\n",
      "4509/4509 [==============================] - 27s 6ms/step - loss: 0.0610\n",
      "****GAN step:  3\n",
      "Train Generator at round  0\n",
      "Reward: 0.048, Episode end\n",
      "Loss:  0.793505\n",
      "[1, 9300, 2613, 147, 43, 187, 2902, 18, 1183, 8420, 21, 21, 44, 21, 6, 2]\n",
      "<BOS> cracks giant way my best easter on himself howie you you thank you ! <EOS>\n",
      "[1, 2354, 4996, 86, 74, 166, 783, 183, 392, 5, 1773, 321, 1954, 111, 1318, 2]\n",
      "<BOS> fighters confirm or many other dead ever wrong the challenge congress fixed them common <EOS>\n",
      "[1, 5983, 7393, 1228, 17, 548, 760, 3691, 1096, 6, 2, 0, 0, 0, 0, 0]\n",
      "<BOS> gazette shout disgusting - complete television nonsense list ! <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "--------------------------------------------------------------------------------\n",
      "Train Discriminator at round  0\n",
      "Start Generating 5000 sentences\n",
      "Num of all training:  582769\n",
      "Num of all validation:  0\n",
      "Epoch 1/1\n",
      "4552/4552 [==============================] - 27s 6ms/step - loss: 0.0513\n",
      "****GAN step:  4\n",
      "Train Generator at round  0\n",
      "Reward: 0.032, Episode end\n",
      "Loss:  0.56990963\n",
      "[1, 6850, 1449, 11, 4, 6, 19, 297, 2, 0, 0, 0, 0, 0, 0, 0]\n",
      "<BOS> slats memorial in . ! # maga <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "[1, 9778, 37, 135, 6640, 13, 395, 719, 325, 3107, 220, 3981, 190, 921, 5350, 3239]\n",
      "<BOS> publicized was her condominiums ' ve giving such @netanyahu record swearing russia except lawsuits penalty\n",
      "[1, 3154, 48, 4836, 58, 1513, 8294, 17, 343, 8174, 485, 278, 193, 1391, 1224, 177]\n",
      "<BOS> 1000000 just employed ) @nbcnews @surveyusa - ted nose billion dollars strong expensive crisis military\n",
      "--------------------------------------------------------------------------------\n",
      "Train Discriminator at round  0\n",
      "Start Generating 5000 sentences\n"
     ]
    }
   ],
   "source": [
    "trainer.train(steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
